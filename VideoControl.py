import cv2  # type: ignore
import mediapipe as mp  # type: ignore
import numpy as np  # type: ignore
import os
import time
from keras.models import load_model  # type: ignore
import ctypes
import vlc   # type: ignore

# Add VLC installation directory to PATH to ensure libvlc.dll and dependencies are found
vlc_installation_path = r"C:\\Program Files\\VideoLAN\\VLC"
os.environ["PATH"] += os.pathsep + vlc_installation_path

# Ensure libvlc.dll is loaded correctly
try:
    ctypes.CDLL(os.path.join(vlc_installation_path, "libvlc.dll"))
except OSError as e:
    print(f"‚ùå Failed to load libvlc.dll: {e}")
    exit(1)

# Load m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
model = load_model(r'D:\VideoGestureControl\gesture_model_v8.h5')

# Nh√£n t∆∞∆°ng ·ª©ng v·ªõi output c·ªßa m√¥ h√¨nh
gesture_labels = ['Next', 'Pause', 'Play', 'Start']
# Kh·ªüi t·∫°o MediaPipe
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
)

# Chuy·ªÉn landmarks th√†nh vector 42 chi·ªÅu (x, y)
def landmarks_to_vector(hand_landmarks):
    vector = []
    for lm in hand_landmarks.landmark[:21]:  # Ch·ªâ l·∫•y 21 ƒëi·ªÉm ƒë·∫ßu ti√™n
        vector.extend([lm.x, lm.y])
    return np.array(vector, dtype=np.float32)

# D·ª± ƒëo√°n c·ª≠ ch·ªâ t·ª´ landmark
# Th√™m debug ƒë·ªÉ ki·ªÉm tra d·ª± ƒëo√°n

def predict_gesture(hand_landmarks):
    if hand_landmarks is None:
        return None  # Tr·∫£ v·ªÅ None n·∫øu kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c

    input_vector = landmarks_to_vector(hand_landmarks).reshape(1, 42)  # ƒê·∫£m b·∫£o reshape th√†nh (1, 42)
    predictions = model.predict(input_vector, verbose=0)
    #print(f"üîç Raw predictions: {predictions}")  # Debug: In ra d·ª± ƒëo√°n th√¥

    predicted_index = np.argmax(predictions)
    gesture = gesture_labels[predicted_index]
    print(f"‚úÖ Gesture: {gesture} (Index: {predicted_index})")

    with open("gesture_log.txt", "a") as log:
        log.write(f"{time.ctime()}: {gesture}\n")
    return gesture

# Th√™m bi·∫øn l∆∞u tr·ªØ gesture tr∆∞·ªõc ƒë√≥
previous_gesture = None

# Ph√°t video v·ªõi ƒëi·ªÅu khi·ªÉn b·∫±ng c·ª≠ ch·ªâ
# S·ª≠ d·ª•ng VLC ƒë·ªÉ ph√°t video c√≥ √¢m thanh
def play_video(path):
    global gesture_state, video_index, previous_gesture

    # T·∫°o ƒë·ªëi t∆∞·ª£ng VLC
    player = vlc.MediaPlayer(path)
    player.play()
    player.video_set_scale(1.5)  # TƒÉng k√≠ch th∆∞·ªõc video l√™n l·∫ßn
    
    while True:
        if gesture_state == "Pause":
            player.set_pause(1)  # T·∫°m d·ª´ng video v√† √¢m thanh
        else:
            player.set_pause(0)  # Ti·∫øp t·ª•c ph√°t video

        # X·ª≠ l√Ω camera ngay c·∫£ khi video ƒëang t·∫°m d·ª´ng
        ret_cam, frame_cam = cap.read()
        if not ret_cam:
            print("‚ùå Cannot read webcam.")
            break

        frame_cam = cv2.flip(frame_cam, 1)
        rgb = cv2.cvtColor(frame_cam, cv2.COLOR_BGR2RGB)
        result = hands.process(rgb)

        if result.multi_hand_landmarks:
            for hand_landmarks in result.multi_hand_landmarks:
                mp_drawing.draw_landmarks(frame_cam, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                gesture = predict_gesture(hand_landmarks)

                # Ch·ªâ x·ª≠ l√Ω khi gesture kh√°c v·ªõi gesture tr∆∞·ªõc ƒë√≥
                if gesture != previous_gesture:
                    previous_gesture = gesture
                    if gesture == "Start":
                        gesture_state = "Play"
                    elif gesture == "Play":
                        gesture_state = "Play"
                    elif gesture == "Pause":
                        gesture_state = "Pause"
                    elif gesture == "Next":
                        video_index = (video_index + 1) % len(video_list)
                        player.stop()
                        return  # chuy·ªÉn video m·ªõi
                        
        # Gi·∫£m k√≠ch th∆∞·ªõc khung h√¨nh camera tr∆∞·ªõc khi hi·ªÉn th·ªã
        frame_cam = cv2.resize(frame_cam, (320, 240))
        cv2.putText(frame_cam, f"Gesture: {gesture_state}", (10, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)
        cv2.imshow("Gesture Camera", frame_cam)
        # ƒê∆∞a c·ª≠a s·ªï webcam xu·ªëng g√≥c ph·∫£i d∆∞·ªõi m√†n h√¨nh
        user32 = ctypes.windll.user32
        screen_width = user32.GetSystemMetrics(0)
        screen_height = user32.GetSystemMetrics(1)
        cam_width, cam_height = 480, 360
        cv2.moveWindow("Gesture Camera", screen_width - cam_width, screen_height - cam_height)

        key = cv2.waitKey(10) & 0xFF
        if key == 27:  # ESC
            cap.release()
            player.stop()
            cv2.destroyAllWindows()
            exit()
        elif key == ord('n'):
            video_index = (video_index + 1) % len(video_list)
            player.stop()
            return  # chuy·ªÉn video m·ªõi

# T·∫£i danh s√°ch video
video_folder = r'D:\VideoGestureControl\Video'
video_list = [os.path.join(video_folder, f) for f in os.listdir(video_folder) if f.endswith('.mp4')]

# Kh·ªüi t·∫°o webcam
cap = cv2.VideoCapture(0)

# Th·ª≠ m·ªü camera v·ªõi c√°c ID kh√°c nhau n·∫øu ID 0 kh√¥ng ho·∫°t ƒë·ªông
camera_id = 0
while not cap.isOpened() and camera_id < 5:
    print(f"‚ö†Ô∏è Camera ID {camera_id} kh√¥ng ho·∫°t ƒë·ªông. Th·ª≠ ID ti·∫øp theo...")
    camera_id += 1
    cap = cv2.VideoCapture(camera_id)

if not cap.isOpened():
    print("‚ùå Kh√¥ng th·ªÉ m·ªü b·∫•t k·ª≥ camera n√†o. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi ho·∫∑c quy·ªÅn truy c·∫≠p.")
    exit(1)

print(f"‚úÖ Camera ƒë√£ ƒë∆∞·ª£c m·ªü th√†nh c√¥ng v·ªõi ID {camera_id}.")

# X√≥a logic ki·ªÉm tra khung h√¨nh t·ª´ camera

gesture_state = "Pause"
video_index = 0

# V√≤ng l·∫∑p ch√≠nh
while True:
    print(f"‚ñ∂ Playing video: {video_list[video_index]}\n")
    play_video(video_list[video_index])